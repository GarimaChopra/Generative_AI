{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarimaChopra/Generative_AI/blob/main/Final_project_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyPMuvdrisJy"
      },
      "outputs": [],
      "source": [
        "#Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhhU9bPQDrzJ"
      },
      "outputs": [],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF52bXcylTE7"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('https://raw.githubusercontent.com/GarimaChopra/Generative_AI/main/Research%20Project/Indian_Food_Cleaned_Dataset1.csv')\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07Txw_ZznkBo"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mTV_OiZnxyH"
      },
      "outputs": [],
      "source": [
        "#Dropping columns not needed\n",
        "columns_to_drop = [ 'URL','TranslatedIngredients','TotalTimeInMins', 'image-url',]\n",
        "df = df.drop(columns = columns_to_drop).dropna()\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfzo2xzdoucF"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ospoRgXPpIut"
      },
      "outputs": [],
      "source": [
        " #Dropping irrelevant receipes from other cuisines\n",
        "cuisines_to_drop = ['Mexican', 'Italian Recipes', 'Thai', 'Chinese', 'Asian', 'Middle Eastern', 'European',\n",
        "                   'Arab', 'Japanese', 'Vietnamese', 'British', 'Greek', 'French', 'Mediterranean', 'Sri Lankan',\n",
        "                   'Indonesian', 'African', 'Korean', 'American', 'Carribbean', 'World Breakfast', 'Malaysian', 'Dessert',\n",
        "                   'Afghan', 'Snack', 'Jewish', 'Brunch', 'Lunch', 'Continental', 'Fusion']\n",
        "\n",
        "df = df[~df['Cuisine'].isin(cuisines_to_drop)]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QqjwTdwpUdc"
      },
      "outputs": [],
      "source": [
        "#Cleaning Receipes Name\n",
        "import re\n",
        "def clean_recipe(recipe_str):\n",
        "\n",
        "    # Remove content inside parentheses\n",
        "    cleaned = re.sub(r'\\(.*?\\)', '', recipe_str)\n",
        "\n",
        "    # Extract the string before \" - \" pattern\n",
        "    cleaned = cleaned.split(\" - \")[0]\n",
        "    cleaned = cleaned.split(\" | \")[0]\n",
        "    return cleaned.strip()\n",
        "\n",
        "df[\"TranslatedRecipeName\"] = df[\"TranslatedRecipeName\"].apply(clean_recipe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DndtJAPpaNr"
      },
      "outputs": [],
      "source": [
        "df.sample(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7bTY2wSpfi-"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsmkKFF9pic3"
      },
      "outputs": [],
      "source": [
        "#Cleaning Ingredients\n",
        "def clean_ingredients(ingredient_str):\n",
        "    # Split the input ingredient string by commas\n",
        "    ingredients = ingredient_str.split(',')\n",
        "    # List of descriptors to remove from ingredient list\n",
        "    descriptors = [\n",
        "        'teaspoon', 'tablespoon', 'cup','cups','as per use', 'grams', 'pieces', 'sliced', 'chopped',\n",
        "        'finely', 'diced', 'desi', 'gms', 'tbsp', 'tsp', 'ml', 'inch', 'large',\n",
        "        'medium', 'small', 'shredded', 'to taste', 'roughly', 'fresh', 'peeled',\n",
        "        'de seeded', 'deseeded', 'crushed', 'whole', 'cubes', 'cube', 'round',\n",
        "        'grated', 'powder', 'optional', 'dry', 'washed', 'soaked', 'cooked',\n",
        "        'uncooked', 'ripe', 'unripe', 'frozen', 'thin', 'thick', 'cleaned', 'thinly',\n",
        "        'for', 'into', 'and', 'as per taste', 'to', 'cut', 'overnight', 'leaves',\n",
        "        'into strips', 'ti','minutes','whisked','salt','to taste','taste','Salt','tablespoons','Red','Green','as','Haldi','Jeera','Dhaniya'\n",
        "        'oil','or','a','black','dry','masala','deep','Oil','Teaspoon','Leaves','powder','Dry','Whole','fresh',\n",
        "        'Garam','gram','seeds','Required','water','Water','green','white','per','pinch','paste','slit','&','leaf','boiled',\n",
        "        'yellow','garnish','curry','minced','mashed','optional','few','fresh','required','in','use','of','sprig','Badi','Kala','red'\n",
        "    ]\n",
        "\n",
        "    cleaned_ingredients = []\n",
        "    for ingredient in ingredients:\n",
        "        # Remove digits\n",
        "        ingredient = re.sub(r'\\d+', '', ingredient)\n",
        "        # Replace dashes and slashes with spaces\n",
        "        ingredient = re.sub(r'[-/]', ' ', ingredient)\n",
        "        ingredient=re.sub(r\"[\\([{})\\]]\", \"\", ingredient)\n",
        "\n",
        "        # Remove descriptor words\n",
        "        ingredient = ' '.join(\n",
        "            [word for word in ingredient.split() if word not in descriptors]\n",
        "        )\n",
        "        cleaned_ingredients.append(ingredient.strip())\n",
        "    return ', '.join(cleaned_ingredients)\n",
        "\n",
        "# Fill NA values in 'Cleaned-Ingredients' column\n",
        "df['Cleaned-Ingredients'].fillna(\"\", inplace=True)\n",
        "# Filter out rows containing Devanagari script characters\n",
        "df = df[~df[\"Cleaned-Ingredients\"].str.contains(r'[ऀ-ॿ]')]\n",
        "df[\"Cleaned-Ingredients\"] = df[\"Cleaned-Ingredients\"].apply(clean_ingredients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlbCLhIIp4at"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRkx7PcNp84C"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "vocabulary = nltk.FreqDist()\n",
        "\n",
        "for ingredients in df['Cleaned-Ingredients']:\n",
        "    ingredients = ingredients.split()\n",
        "    vocabulary.update(ingredients)\n",
        "# initialize nltk's lemmatizer\n",
        "for word, frequency in vocabulary.most_common(200):\n",
        "    print(f'{word};{frequency}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUfw27n5ta6F"
      },
      "outputs": [],
      "source": [
        "df = df[~df[\"TranslatedInstructions\"].str.contains(r'[ऀ-ॿ]')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wrKLlj4tfk7"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QxMm_kNuvZ3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg0n6EolyXcL"
      },
      "outputs": [],
      "source": [
        "model_name = 'gpt2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqr5u0FvyVJ3"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name,\n",
        "                                              bos_token='<|startoftext|>',\n",
        "                                              eos_token='<|endoftext|>',\n",
        "                                              unk_token='<|unknown|>',\n",
        "                                              pad_token='<|pad|>'\n",
        "                                             )\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBxy91ZH02pr"
      },
      "outputs": [],
      "source": [
        "model_save_path = '/content/sample_data/Research'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mbl5oRWCBUr"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKFI-e1vxL5m"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_tokens_to_ids(['<|pad|>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQZoGB5PzATS"
      },
      "outputs": [],
      "source": [
        "def generate(prompt):\n",
        "    inputs = tokenizer.encode_plus(prompt, return_tensors='pt')\n",
        "    output = model.generate(inputs,max_length=256,do_sample=True,pad_token_id=50259)\n",
        "    print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBUJoFd0zFeF"
      },
      "outputs": [],
      "source": [
        "tokenizer.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFbjVy_WzKFb"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_tokens_to_ids(['<|startoftext|>'],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB4480px9tF-"
      },
      "outputs": [],
      "source": [
        "def print_recipe(idx):\n",
        "    print(f\"{df['Ingredients'][idx]}\\n\\n{df['instructions'][idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIqxBdrT9_er"
      },
      "outputs": [],
      "source": [
        "def form_string(ingredient,instruction):\n",
        "    s = f\"<|startoftext|>Ingredients:\\n{ingredient.strip()}\\n\\nInstructions:\\n{instruction.strip()}<|endoftext|>\"\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muUnZ0kC-UyE"
      },
      "outputs": [],
      "source": [
        "data = df.apply(lambda x:form_string(x['Cleaned-Ingredients'],x['TranslatedInstructions']),axis=1).to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27_FU1rfAibT"
      },
      "outputs": [],
      "source": [
        "train_size = 0.85\n",
        "train_len = int(train_size * len(data))\n",
        "train_data = data[:train_len]\n",
        "val_data = data[train_len:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S4EByWbAm3N"
      },
      "outputs": [],
      "source": [
        "class RecipeDataset:\n",
        "    def __init__(self,data):\n",
        "        self.data = data\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for item in tqdm(data):\n",
        "            encodings = tokenizer.encode_plus(item,\n",
        "                                              truncation=True,\n",
        "                                              padding='max_length',\n",
        "                                              max_length=1024,\n",
        "                                              return_tensors='pt'\n",
        "                                             )\n",
        "            self.input_ids.append(torch.squeeze(encodings['input_ids'],0))\n",
        "            self.attn_masks.append(torch.squeeze(encodings['attention_mask'],0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4vwnsry0drR"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([item[0] for item in batch]),\n",
        "        'attention_mask': torch.stack([item[1] for item in batch]),\n",
        "        'labels': torch.stack([item[0] for item in batch])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKGBaRcCzUf0"
      },
      "outputs": [],
      "source": [
        "train_ds = RecipeDataset(train_data)\n",
        "val_ds = RecipeDataset(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDSNwtDFzdzD"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(output_dir=model_save_path,\n",
        "                         per_device_train_batch_size=2,\n",
        "                         per_device_eval_batch_size=2,\n",
        "                         gradient_accumulation_steps=2,\n",
        "                         report_to='none',\n",
        "                         num_train_epochs=1,\n",
        "                         save_strategy='no'\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e16S7PwGzlX0"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.AdamW(model.parameters(),lr=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,20,eta_min=1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZy8oRr7zp25"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model,\n",
        "                  args,\n",
        "                  train_dataset=train_ds,\n",
        "                  eval_dataset=val_ds,\n",
        "                  data_collator=collate_fn,\n",
        "                  optimizers=(optim,scheduler)\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "5VPALFFdqJBC",
        "outputId": "c1ee3153-7c55-4099-9726-3dd2d3187ad5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='862' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  2/862 : < :, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAcyyL8lrWTh"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUigyI_Tra_7"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaiqE9reHGAT"
      },
      "outputs": [],
      "source": [
        "pl = pipeline(task='text-generation',model= '/content/sample_data/Research')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCs04tEg4NYX"
      },
      "outputs": [],
      "source": [
        "\n",
        "output_dir = './model_80-20-train/'\n",
        "\n",
        "# # Create output directory if needed\n",
        "# if not os.path.exists(output_dir):\n",
        "#     os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2H-nSzf4cxB"
      },
      "outputs": [],
      "source": [
        "output_dir = './model_80-20-train/'\n",
        "#Load a trained model and vocabulary that you have fine-tuned\n",
        "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QMF8Z6qHeog"
      },
      "outputs": [],
      "source": [
        "def create_prompt(ingredients):\n",
        "    ingredients = ','.join([x.strip().lower() for x in ingredients.split(',')])\n",
        "    ingredients = ingredients.strip().replace(',','\\n')\n",
        "    s = f\"<|startoftext|>Ingredients:\\n{ingredients}\\n\"\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZQunRZRJzax"
      },
      "outputs": [],
      "source": [
        "ingredients = ['Rice,CHICKEN,LAMB,paneer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk9Cvp2nJ6nP"
      },
      "outputs": [],
      "source": [
        "for ing in ingredients:\n",
        "    prompt = create_prompt(ing)\n",
        "    print(pl(prompt,\n",
        "         max_new_tokens=512,\n",
        "         penalty_alpha=0.6,\n",
        "         top_k=4,\n",
        "         pad_token_id=50259\n",
        "        )[0]['generated_text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCKT8KqsytQcagSJds8RHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}